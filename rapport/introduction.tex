\bibliographystyle{plain}
\section*{introduction}
This article \cite{article} presents an algorithm to learn non-linear stochastic dynamical system with incomplete observation. This algorithm is an instance of EM algorithm, estimating the distributions of the hidden variables in the E-Step and learning the parameters of the non-linear functions in the M-Step. Actually, it uses very specific assumptions about the non linearities, namely that they can be decomposed as linear combination of Radial Basis Functions plus an affine term. This assumption allows to compute efficiently the exact M-steps in order to avoid sampling issues which can be very costly if they need to be done at each step.

The article provides a few keys, as to derive the machinery, but most of the computations are left to the reader. With the help of the book of Simon Haykin \cite{SimonHaykin}, which the article is from,  and the lectures notes, we managed to derive the computations in the most general case. Firstly, we present them, following the guideline of the article, and then present our numerical results on simulated data.



% It is regrettable that none of the computations that are necessary for the implementations of the presented algorithm are fully presented, neither for the E-Step nor for the M-Step.
% This article is the sixth chapter of the book "Kalman Filtering and Neural Network" edited by Simon Haykin and the authors mention the chapter one for some of the computations they do not provide.
% We luckily found on the internet a numerical version of that book (which is otherwise available for sale for more than 100 euros) and we found out that the graphical model that the authors use in the article is even not the same as the one presented in chapter one of that book.

% Before summerizing this article, we would like to insist on the fact that we were really disapointed by the quality of that article which spends a lot of time explaining simple things (EM algorithm principle, linear Kalman filter equations) but which provides none of the key computations, neither for the E-Step (Extended Kalman Smoothing) nor for the M-Step (log likelihood maximization).

% It costed us a lot of time and reflexion to understand that we had to do it, and then to do it.
% Fortunatelly, the lecture notes were of great help in that work.
