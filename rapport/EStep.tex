\section{E-Step}

At iteration $k$, in the E-step, given the parameter $\theta^{(k-1)} = \left( \theta_f^{(k-1)}, \theta_g^{(k-1)}, Q^{(k-1)}, R^{(k-1)} \right)$ computed in the iteration $k-1$, we want to infer the sequences $(x_t)_{t=1 \ldots T}$ and $(x_t, x_{t+1})_{t=1 \ldots T-1}$ knowing the output sequence $(y_t)_{t=1 \cdots T}$ and the input sequence $(u_t)_{t=1 \cdots T}$.
In other words we want to compute the conditional probabilities :
\begin{align*}
  &p_{\theta^{(k-1)}}\left(x_t \given y_1, u_1, \ldots, y_T, u_T \right), t=1 \ldots T\\
  &p_{\theta^{(k-1)}}\left(x_t, x_{t+1} \given y_1, u_1, \ldots, y_T, u_T \right), t=1 \ldots T-1\\
\end{align*}

\textbf{Essentially we are able to solve this inference problem (filtering or smoothing) when the dynamic is linear.}
However,  if we use the non-linear dynamics equations with $f$ and $g$, we have no guarantees that, starting from a Gaussian probability for $x_1$, we will have a Gaussian probability for $x_{t}$, and the computations become intractable.
\textbf{To solve this problem, the authors propose to linearize the dynamics equations (\ref{eq:A},\ref{eq:B}) by their first order Taylor expansion at each time step $t$ and at a well-chosen point for filtering ($\tilde{x}_t$) and for smoothing ($\dot{x}_t$) :}
\begin{align*}
  x_{t+1} &= f(\tilde{x}_t, u_t) + A_{\tilde{x}_t} (x_t - \tilde{x}_t) + v_t\\
  A_t &= \frac{\partial f}{\partial x}(\tilde{x}_t, u_t)\\
  y_t &= g(\tilde{x}_t, u_t) + C_{\tilde{x}_t} (x_t - \tilde{x}_t) + w_t\\
  C_t &= \frac{\partial g}{\partial x}(\tilde{x}_t, u_t).\\
\end{align*}

Then the functions are locally linear, thus all probabilities are Gaussian and we can adapt the classical inferences algorithms (\textbf{Kalman filter} for filtering and \textbf{Rauch-Tung-Stribel smoother} for smoothing).\\

This forward-backward approach, involving a linearization at each time step, is called Extented Kalman Smoothing (EKS).
\textbf{One of the important issues is to define $\mathbf{\tilde{x}_t}$ and $\mathbf{\dot{x}_t}$.}

\subsection{Kalman filter}
Let's first recall how Kalman filter proceed with inputs. It is a particular case where $f$ and $g$ are affine (e.g. $I=0$ and $J=0$) :
\begin{align*}
  f(x,u) &= Ax + Bu + b\\
  g(x,u) &= Cx + Du + d\\
\end{align*}
the Kalman filter algorithm computes dynamically the conditional probabilities :
\begin{align*}
  &p_{\theta^{(k-1)}}\left(x_t \given y_1, u_1, \ldots, y_t, u_t \right), t=1 \ldots T\\
  &p_{\theta^{(k-1)}}\left(x_t, x_{t+1} \given y_1, u_1, \ldots, y_t, u_t \right), t=1 \ldots T-1\\
\end{align*}
Putting a gaussian probability on the first state $x_0$ and assuming the \textbf{additive} noise sequences $(w_t)$ and $(v_t)$ are Gaussian, since $f$ and $g$ are affine here, all the probabilities we are dealing with are then Gaussian.
Thus we only need to compute means and covariances.\\

The authors derive the computations of the Kalman filter only in the case where $B,b,D$ and $d$ are all zero.
They didn't give any reasons for these simplifications, but we extended the computations to take those into account.\\
%Is it lazyness?
%Is it intended for pedagoical reason?

As in \cite{Poly}, we denote by $\hat{x}_{t|t}$ the mean of $x_t$ conditioned on $u_1;\cdots;u_t$ and for $\hat{P}_{t|t}$ the covariance of $x_t$ given $u_1;\cdots;u_t$. It is straightforward to adapt these notations in the smoothing case ($\hat{P}_{t|T}$ and $\hat{x}_{t|T}$).
We adopt the following notations :
\begin{align*}
  \hat{x}_{t \given t} &= \mathbb{E}_p \left(x_t \given y_1, \ldots , y_t, u_1, \ldots , u_t \right) \\
  \hat{x}_{t+1 \given t} &= \mathbb{E}_p \left(x_{t+1} \given y_1, \ldots , y_t, u_1, \ldots , u_t \right) \\
  \hat{P}_{t \given t} &= \mathbb{E}_p \left((x_t - \hat{x}_{t \given t})(x_t - \hat{x}_{t \given t})^{\top} \given y_1, \ldots , y_t, u_1, \ldots , u_t \right) \\
  \hat{P}_{t+1 \given t} &= \mathbb{E}_p \left((x_{t+1} - \hat{x}_{t+1 \given t})(x_{t+1} - \hat{x}_{t+1 \given t})^{\top} \given y_1, \ldots , y_t, u_1, \ldots , u_t \right) \\
  \hat{P}_{t,t+1 \given t} &= \mathbb{E}_p \left((x_t - \hat{x}_{t \given t})(x_{t+1} - \hat{x}_{t+1 \given t})^{\top} \given y_1, \ldots , y_t, u_1, \ldots , u_t \right) \\
\end{align*}
As already mentioned, computing these quantities amounts to infer the conditional probability distribution of the hidden states since we are working with gaussians distribution. The Kalman filter gives us a recursion equation to compute these quantities :
\begin{itemize}
  \item \textbf{Initialization}\\
    \begin{align*}
      \hat{x}_{1 \given 0} &= 0\\
      \hat{P}_{1 \given 0} &= \Sigma_0\\
    \end{align*}
  \item \textbf{Forward recursion}\\
    For $t=1 \ldots T$ :
    \begin{align*}
      \hat{x}_{t+1 \given t} &= A \hat{x}_{t \given t} + B u_t + b\\
      \hat{P}_{t+1 \given t} &= A \hat{P}_{t \given t} A^{\top} + Q\\
      \hat{P}_{t,t+1 \given t} &= \hat{P}_{t \given t}A^{\top}\\
      K_{t+1} &= \hat{P}_{t+1 \given t}C^{\top}(C \hat{P}_{t+1 \given t} C^{\top} + R^{(k-1)})^{-1}\\
      \hat{x}_{t+1 \given t+1} &= \hat{x}_{t+1 \given t} + K_{t+1} \left(y_{t+1} - (C\hat{x}_{t+1 \given t} + D u_t + d) \right)\\
      \hat{P}_{t+1 \given t+1} &= \hat{P}_{t+1 \given t} - K_{t+1} C \hat{P}_{t+1 \given t}\\
    \end{align*}
We compute the matrix $K_{t}$ (Kalman gain matrix) to simplify the computations.
\end{itemize}

In our case, $f$ and $g$ are not linear.
So for $t=1..T$ we linearize our functions $f$ and $g$ around the point $\tilde{x}_t$ and then we can apply the recursion equation below.
We define $\tilde{x}_t$ as following :
\begin{align*}
  \tilde{x}_1 &= \hat{x}_{1 \given 0}\\
  \tilde{x}_{t+1} &= f(\hat{x}_{t \given t}, u_t)\\
\end{align*}
Then, the dynamics become :
\begin{align*}
  A_t &= \frac{\partial f}{\partial x}(\tilde{x}_t, u_t) = A + \sum_{i=1}^I \rho_i(\tilde{x}_t) h_i (\tilde{x}_t - c_i)^{\top} S_i^{-1}\\
  C_t &= \frac{\partial g}{\partial x}(\tilde{x}_t, u_t) = C + \sum_{j=1}^J \rho'_j(\tilde{x}_t) k_j (\tilde{x}_t - c'_i)^{\top} {S'_j}^{-1}\\
  x_{t+1} &= A_{t} x_t + (f(\tilde{x}_t, u_t) - A_{t} \tilde{x}_t) + v_t\\
  y_t &= C_{t} x_t + (g(\tilde{x}_t, u_t) - C_{t}  \tilde{x}_t) + w_t\\
\end{align*}
in the recursion formula, we just have to replace the matrices $A$ and $C$ by the matrices $A_t$ and $C_t$  and we replace the vectors $b$ and $d$ by the vectors $b_t$ and $d_t$ :
\begin{align*}
  b_t &= f(\tilde{x}_t, u_t) - A_{\tilde{x}_t} \tilde{x}_t\\
  d_t &= g(\tilde{x}_t, u_t) - C_{\tilde{x}_t} \tilde{x}_t\\
\end{align*}
and we get the conditionnal probabilities of interest :
\begin{align*}
  &p_{\theta^{(k-1)}}\left(x_t \given y_1, u_1, \ldots, y_t, u_t \right), t=1 \ldots T\\
  &p_{\theta^{(k-1)}}\left(x_t, x_{t+1} \given y_1, u_1, \ldots, y_t, u_t \right), t=1 \ldots T-1\\
\end{align*}

\subsection{Rauch-Tung-Stribel smoother}
The authors provide no computations for the RTS.
So all the computations done here were inspired from the computations done in the lecture notes.

If the case where $f$ and $g$ are affine, the RTS smoother algorithm uses the conditionnal probabilities computed with Kalman filter to compute the conditional probabilities :
\begin{align*}
  &p_{\theta^{(k-1)}}\left(x_t \given y_1, u_1, \ldots, y_T, u_T \right), t=1 \ldots T\\
  &p_{\theta^{(k-1)}}\left(x_t, x_{t+1} \given y_1, u_1, \ldots, y_T, u_T \right), t=1 \ldots T-1\\
\end{align*}
Once again, theses probabilities are Gaussian and only we need to compute their means and covariances.

The following notations follow the same rule as for Kalman filtering:
\begin{align*}
  \hat{x}_{t \given T} &= \mathbb{E}_p(x_t \given y_1, \ldots , y_T, u_1, \ldots , u_T) \\
  \hat{P}_{t \given T} &= \mathbb{E}_p \left((x_t - \hat{x}_{t \given T})(x_t - \hat{x}_{t \given T})^{\top} \given y_1, \ldots , y_T, u_1, \ldots , u_T \right) \\
  \hat{P}_{t,t+1 \given T} &= \mathbb{E}_p \left((x_t - \hat{x}_{t \given T})(x_{t+1} - \hat{x}_{t+1 \given T})^{\top} \given y_1, \ldots , y_T, u_1, \ldots , u_T \right) \\
\end{align*}
With these quanities, we can compute the conditionnal probabilities of interest.
The Kalman filter gives us a recursion equation to compute these quanities :
\begin{itemize}
  \item \textbf{Initialization}\\
    For the initialization, we need to run a Kalman filter to get the quantities $\hat{x}_{t \given t}, \hat{P}_{t \given t}$ for $t=1 \ldots T$ and $\hat{x}_{t+1 \given t}, \hat{P}_{t+1 \given t}, \hat{P}_{t,t+1 \given t}$ for $t=a \ldots T-1$.
  \item \textbf{Backward recursion}\\
    For $t=T-1 \ldots 1$ :
    \begin{align*}
      L_t &= \hat{P}_{t \given t}A^{\top}\hat{P}_{t+1 \given t}^{-1}\\
      \hat{x}_{t \given T} &= \hat{x}_{t \given t} + L_t(\hat{x}_{t+1 \given T} - \hat{x}_{t+1 \given t})\\
      \hat{P}_{t \given T} &= \hat{P}_{t \given t} + L_t(\hat{P}_{t+1 \given T} - \hat{P}_{t+1 \given t})L_t^{\top}\\
      \hat{P}_{t,t+1 \given T} &= \hat{P}_{t,t+1 \given t} - (\hat{x}_{t \given t} - \hat{x}_{t \given T})(\hat{x}_{t+1 \given t} - \hat{x}_{t+1 \given T})^{\top}\\
    \end{align*}
    We compute the matrix $L_{t}$ to simplify the computations.
\end{itemize}

In our case, $f$ and $g$ are not linear.
We linearize them around the point $\dot{x}_t$ and then we can apply the recursion equation below.
We define  as follow :
\begin{align*}
  \dot{x}_t &= \hat{x}_{t \given t} = \mathbb{E}_p(x_t \given y_1, \ldots , y_t, u_1, \ldots , u_t) \\
\end{align*}
As we see, $\dot{x}_t$ can be seen as the mean of probability  $p_{\theta^{(k-1)}}\left(x_t \given y_1, u_1, \ldots, y_t, u_T \right)$ computed by the Kalman filter.
We also see that in the recursion forumla, the matrix $C$ and the vectors $b$ and $d$ do not appear.
So we just have to replace the matrix $A$ by the matrix $A_t$ :
\begin{align*}
  A_t &= \frac{\partial f}{\partial x}(\dot{x}_t, u_t)\\
\end{align*}
The computation of $A_t$ in term of our parameters $\theta_f^{(k-1)}$ and $\theta_g^{(k-1)}$ and of the centers and width of our RBF functions is the same as in the Kalman filter, replacing $\tilde{x}$ by $\dot{x}$.

So we end up with the means and covariance matrices of the probabilities
\begin{align*}
  &p_{\theta^{(k-1)}}\left(x_t \given y_1, u_1, \ldots, y_T, u_T \right), t=1 \ldots T\\
  &p_{\theta^{(k-1)}}\left(x_t, x_{t+1} \given y_1, u_1, \ldots, y_T, u_T \right), t=1 \ldots T-1\\
\end{align*}
\textbf{which are exactly what we wanted to compute during the E-step.
So we can move to the M-step.}
